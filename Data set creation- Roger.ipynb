{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.dates as md\n",
    "\n",
    "\n",
    "class CreateDataset():\n",
    "\n",
    "    base_dir = '.\\csv-participant-one'\n",
    "    granulairity = 0\n",
    "    data_table = None\n",
    "\n",
    "    def __init__(self, base_dir, granularity):\n",
    "        self.base_dir = base_dir\n",
    "        self.granularity = granularity\n",
    "\n",
    "    # Create an initial data table with entries from start till end time, with steps\n",
    "    # of size granularity. Granularity is specified in milliseconds\n",
    "    def create_timestamps(self, start_time, end_time):\n",
    "        return pd.date_range(start_time, end_time, freq=str(self.granularity)+'ms')\n",
    "\n",
    "    def create_dataset(self, start_time, end_time, cols, prefix):\n",
    "        c = copy.deepcopy(cols)\n",
    "        if not prefix == '':\n",
    "            for i in range(0, len(c)):\n",
    "                c[i] = str(prefix) + str(c[i])\n",
    "        timestamps = self.create_timestamps(start_time, end_time)\n",
    "        self.data_table = pd.DataFrame(index=timestamps, columns=c)\n",
    "\n",
    "    # Add numerical data, we assume timestamps in the form of nanoseconds from the epoch\n",
    "    def add_numerical_dataset(self, file, timestamp_col, value_cols, aggregation='avg', prefix=''):\n",
    "        dataset = pd.read_csv(self.base_dir + file, skipinitialspace=True)\n",
    "\n",
    "        # Convert timestamps to dates\n",
    "        dataset[timestamp_col] = pd.to_datetime(dataset[timestamp_col])\n",
    "\n",
    "        # Create a table based on the times found in the dataset\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[timestamp_col]), max(dataset[timestamp_col]), value_cols, prefix)\n",
    "        else:\n",
    "            for col in value_cols:\n",
    "                self.data_table[str(prefix) + str(col)] = np.nan\n",
    "\n",
    "        # Over all rows in the new table\n",
    "        for i in range(0, len(self.data_table.index)):\n",
    "            # Select the relevant measurements.\n",
    "            relevant_rows = dataset[\n",
    "                (dataset[timestamp_col] >= self.data_table.index[i]) &\n",
    "                (dataset[timestamp_col] < (self.data_table.index[i] +\n",
    "                                           timedelta(milliseconds=self.granularity)))\n",
    "            ]\n",
    "            for col in value_cols:\n",
    "                # Take the average value\n",
    "                if len(relevant_rows) > 0:\n",
    "                    if aggregation == 'avg':\n",
    "                        self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.average(relevant_rows[col])\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n",
    "                else:\n",
    "                    self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.nan\n",
    "\n",
    "    # Remove undesired value from the names.\n",
    "    def clean_name(self, name):\n",
    "        return re.sub('[^0-9a-zA-Z]+', '', name)\n",
    "\n",
    "    # Add data in which we have rows that indicate the occurrence of a certain event with a given start and end time.\n",
    "    # 'aggregation' can be 'sum' or 'binary'.\n",
    "    def add_event_dataset(self, file, start_timestamp_col, end_timestamp_col, value_col, aggregation='sum'):\n",
    "        dataset = pd.read_csv(self.base_dir + file)\n",
    "\n",
    "        # Convert timestamps to datetime.\n",
    "        dataset[start_timestamp_col] = pd.to_datetime(dataset[start_timestamp_col])\n",
    "        dataset[end_timestamp_col] = pd.to_datetime(dataset[end_timestamp_col])\n",
    "\n",
    "        # Clean the event values in the dataset\n",
    "        dataset[value_col] = dataset[value_col].apply(self.clean_name)\n",
    "        event_values = dataset[value_col].unique()\n",
    "\n",
    "        # Add columns for all possible values (or create a new dataset if empty), set the default to 0 occurrences\n",
    "        if self.data_table is None:\n",
    "            self.create_dataset(min(dataset[start_timestamp_col]), max(dataset[end_timestamp_col]), event_values, value_col)\n",
    "        for col in event_values:\n",
    "            self.data_table[(str(value_col) + str(col))] = 0\n",
    "\n",
    "        # Now we need to start counting by passing along the rows....\n",
    "        for i in range(0, len(dataset.index)):\n",
    "            # identify the time points of the row in our dataset and the value\n",
    "            start = dataset[start_timestamp_col][i]\n",
    "            end = dataset[end_timestamp_col][i]\n",
    "            value = dataset[value_col][i]\n",
    "            border = (start - timedelta(milliseconds=self.granularity))\n",
    "\n",
    "            # get the right rows from our data table\n",
    "            relevant_rows = self.data_table[(start <= (self.data_table.index +timedelta(milliseconds=self.granularity))) & (end > self.data_table.index)]\n",
    "\n",
    "            # and add 1 to the rows if we take the sum\n",
    "            if aggregation == 'sum':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] += 1\n",
    "            # or set to 1 if we just want to know it happened\n",
    "            elif aggregation == 'binary':\n",
    "                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] = 1\n",
    "            else:\n",
    "                raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n",
    "\n",
    "    # This function returns the column names that have one of the strings expressed by 'ids' in the column name.\n",
    "    def get_relevant_columns(self, ids):\n",
    "        relevant_dataset_cols = []\n",
    "        cols = list(self.data_table.columns)\n",
    "\n",
    "        for id in ids:\n",
    "            relevant_dataset_cols.extend([col for col in cols if id in col])\n",
    "\n",
    "        return relevant_dataset_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(int 'column, fraction missing values, mean, standard deviation, min, max')? (util.py, line 23)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2862\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-e7aa1c26a498>\"\u001b[1;36m, line \u001b[1;32m18\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from util import util\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\roger\\Documents\\GitHub\\ML4QS_G49\\util\\util.py\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    print 'column, fraction missing values, mean, standard deviation, min, max'\u001b[0m\n\u001b[1;37m                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(int 'column, fraction missing values, mean, standard deviation, min, max')?\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 2                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "\n",
    "dataset_path = '../datasets/crowdsignals.io/csv-participant-one/'\n",
    "result_dataset_path = './intermediate_datafiles/'\n",
    "\n",
    "# Import the relevant classes.\n",
    "\n",
    "from Chapter2.CreateDataset import CreateDataset\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from util import util\n",
    "import copy\n",
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists(result_dataset_path):\n",
    "    print('Creating result directory: ' + result_dataset_path)\n",
    "    os.makedirs(result_dataset_path)\n",
    "\n",
    "# Chapter 2: Initial exploration of the dataset.\n",
    "\n",
    "# Set a granularity (i.e. how big are our discrete time steps). We start very\n",
    "# coarse grained, namely one measurement per minute, and secondly use four measurements\n",
    "# per second\n",
    "\n",
    "granularities = [60000, 250]\n",
    "datasets = []\n",
    "\n",
    "for milliseconds_per_instance in granularities:\n",
    "\n",
    "    # Create an initial dataset object with the base directory for our data and a granularity\n",
    "    DataSet = CreateDataset(dataset_path, milliseconds_per_instance)\n",
    "\n",
    "    # Add the selected measurements to it.\n",
    "\n",
    "    # We add the accelerometer data (continuous numerical measurements) of the phone and the smartwatch\n",
    "    # and aggregate the values per timestep by averaging the values/\n",
    "    DataSet.add_numerical_dataset('accelerometer_phone.csv', 'timestamps', ['x','y','z'], 'avg', 'acc_phone_')\n",
    "    DataSet.add_numerical_dataset('accelerometer_smartwatch.csv', 'timestamps', ['x','y','z'], 'avg', 'acc_watch_')\n",
    "\n",
    "    # We add the gyroscope data (continuous numerical measurements) of the phone and the smartwatch\n",
    "    # and aggregate the values per timestep by averaging the values/\n",
    "    DataSet.add_numerical_dataset('gyroscope_phone.csv', 'timestamps', ['x','y','z'], 'avg', 'gyr_phone_')\n",
    "    DataSet.add_numerical_dataset('gyroscope_smartwatch.csv', 'timestamps', ['x','y','z'], 'avg', 'gyr_watch_')\n",
    "\n",
    "    # We add the heart rate (continuous numerical measurements) and aggregate by averaging again\n",
    "    DataSet.add_numerical_dataset('heart_rate_smartwatch.csv', 'timestamps', ['rate'], 'avg', 'hr_watch_')\n",
    "\n",
    "    # We add the labels provided by the users. These are categorical events that might overlap. We add them\n",
    "    # as binary attributes (i.e. add a one to the attribute representing the specific value for the label if it\n",
    "    # occurs within an interval).\n",
    "    DataSet.add_event_dataset('labels.csv', 'label_start', 'label_end', 'label', 'binary')\n",
    "\n",
    "    # We add the amount of light sensed by the phone (continuous numerical measurements) and aggregate by averaging again\n",
    "    DataSet.add_numerical_dataset('light_phone.csv', 'timestamps', ['lux'], 'avg', 'light_phone_')\n",
    "\n",
    "    # We add the magnetometer data (continuous numerical measurements) of the phone and the smartwatch\n",
    "    # and aggregate the values per timestep by averaging the values\n",
    "    DataSet.add_numerical_dataset('magnetometer_phone.csv', 'timestamps', ['x','y','z'], 'avg', 'mag_phone_')\n",
    "    DataSet.add_numerical_dataset('magnetometer_smartwatch.csv', 'timestamps', ['x','y','z'], 'avg', 'mag_watch_')\n",
    "\n",
    "    # We add the pressure sensed by the phone (continuous numerical measurements) and aggregate by averaging again\n",
    "    DataSet.add_numerical_dataset('pressure_phone.csv', 'timestamps', ['pressure'], 'avg', 'press_phone_')\n",
    "\n",
    "    # Get the resulting pandas data table\n",
    "\n",
    "    dataset = DataSet.data_table\n",
    "\n",
    "    # Plot the data\n",
    "\n",
    "    DataViz = VisualizeDataset()\n",
    "\n",
    "    # Boxplot\n",
    "    DataViz.plot_dataset_boxplot(dataset, ['acc_phone_x','acc_phone_y','acc_phone_z','acc_watch_x','acc_watch_y','acc_watch_z'])\n",
    "\n",
    "    # Plot all data\n",
    "    DataViz.plot_dataset(dataset, ['acc_', 'gyr_', 'hr_watch_rate', 'light_phone_lux', 'mag_', 'press_phone_', 'label'], ['like', 'like', 'like', 'like', 'like', 'like', 'like','like'], ['line', 'line', 'line', 'line', 'line', 'line', 'points', 'points'])\n",
    "\n",
    "    # And print a summary of the dataset\n",
    "\n",
    "    util.print_statistics(dataset)\n",
    "    datasets.append(copy.deepcopy(dataset))\n",
    "\n",
    "# And print the table that has been included in the book\n",
    "\n",
    "util.print_latex_table_statistics_two_datasets(datasets[0], datasets[1])\n",
    "\n",
    "# Finally, store the last dataset we have generated (250 ms).\n",
    "dataset.to_csv(result_dataset_path + 'chapter2_result.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
